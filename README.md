# conversational-ai-papers
In this seminar, we will explore the fundamentals of conversational AI, from understanding the underlying technologies to hands-on demonstrations of building chatbot applications.



| Title | Paper / Resource | Year | Why is it interesting? | Asignee | Recording | Slides 
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|Large Languague Models|[Llama 2](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT4](https://arxiv.org/pdf/2303.08774.pdf)| 2023 | <details><summary>read why</summary> A review of the greatest and latest LLMs.</details> |  [@ganitk]() |[zoom](TBD)(code)|[slides](TBD) |
|Adapter models|[K-adapters](https://arxiv.org/pdf/2002.01808.pdf), [AdapterHub](https://arxiv.org/pdf/2007.07779.pdf)| 2020 | <details><summary>read why</summary> Model specialization technique which trains only small components on top of the existing model layers.  </details> |  [Presenter]() |[zoom](TBD)(code)|[slides](TBD) |
|Parameter-efficient fine-tuning (PEFT)|[LoRA](https://arxiv.org/pdf/2106.09685.pdf)| 2021 | <details><summary>read why</summary> Fine-tune technique that do not require full model finetuning. The idea behind LoRA is that fine-tuning a foundation model on a downstream task does not require updating all of its parameters. There is a low-dimension matrix that can represent the space of the downstream task with very high accuracy.  </details> |  [Presenter]() |[zoom](TBD)(code)|[slides](TBD) |
|Reinforcement learning from human feedback (RLHF)|[InstructGPT](https://arxiv.org/pdf/2203.02155.pdf)| 2022 | <details><summary>read why</summary> The paper that chatGPT was based on. In the paper the authors use reinforcement learning technique that encoporates human feedback as the reward. Outputs from the 1.3B parameter InstructGPT model are preferred on outputs from the 175B GPT-3 </details> |  [Presenter]() |[zoom](TBD)(code)|[slides](TBD) |
|Retrieval-Augmented Language Modeling (RALM)|[In-Context Retrieval-Augmented Language Models](https://arxiv.org/pdf/2302.00083.pdf)| 2023 | <details><summary>read why</summary>  </details> |  [Presenter]() |[zoom](TBD)(code)|[slides](TBD) |
